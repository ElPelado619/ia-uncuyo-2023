}
df <- read.csv("data/arbolado-mza-dataset.csv")
head(df)
result <- cross_validation(df, num_folds = 5)
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función create_folds
folds <- create_folds(data, num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula<-formula(inclinacion_peligrosa~altura+
circ_tronco_cm+
lat+long+
seccion+
especie)
tree_model<-rpart(train_formula,data)
p<-predict(tree_model,test_data,type='class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = predictions)
# Calcular las métricas de evaluación (por ejemplo, precisión, sensibilidad, especificidad, etc.)
# Puedes personalizar esto según tus necesidades.
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
mean_accuracy <- mean(accuracy_values)
std_accuracy <- sd(accuracy_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy))
}
result <- cross_validation(df, num_folds = 5)
library(rpart)
library(caret)
#Instalar libreria caret
install.packages("caret")
library(rpart)
library(caret)
create_folds <- function(data, num_folds) {
# Obtener el número total de observaciones
total_observations <- nrow(data)
# Calcular el tamaño de cada fold
fold_size <- floor(total_observations / num_folds)
# Crear una lista para almacenar los índices de los folds
fold_list <- list()
# Dividir el dataframe en folds
for (i in 1:num_folds) {
start_index <- (i - 1) * fold_size + 1
end_index <- min(i * fold_size, total_observations)
# Obtener los índices de las observaciones para este fold
fold_indices <- start_index:end_index
# Agregar los índices a la lista
fold_list[[paste("Fold", i)]] <- fold_indices
}
return(fold_list)
}
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ altura + circ_tronco_cm + lat + long + seccion + especie
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación (por ejemplo, precisión, sensibilidad, especificidad, etc.)
# Puedes personalizar esto según tus necesidades.
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
mean_accuracy <- mean(accuracy_values)
std_accuracy <- sd(accuracy_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy))
}
df <- read.csv("data/arbolado-mza-dataset.csv")
result <- cross_validation(df, num_folds = 5)
result <- cross_validation(df, num_folds = 5)
result <- cross_validation(df, num_folds = 5)
result <- cross_validation(df, num_folds = 5)
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ altura + circ_tronco_cm + lat + long + seccion
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación (por ejemplo, precisión, sensibilidad, especificidad, etc.)
# Puedes personalizar esto según tus necesidades.
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
mean_accuracy <- mean(accuracy_values)
std_accuracy <- sd(accuracy_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy))
}
result <- cross_validation(df, num_folds = 5)
# Imprimir la matriz de confusión, media y desviación estándar de la precisión
for (i in 1:length(result$EvaluationMetrics)) {
cat("Fold", i, ":\n")
print(result$EvaluationMetrics[[i]]$ConfusionMatrix)
cat("Accuracy:", result$EvaluationMetrics[[i]]$Accuracy, "\n\n")
}
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ altura + circ_tronco_cm + lat + long + seccion
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación (por ejemplo, precisión, sensibilidad, especificidad, etc.)
# Puedes personalizar esto según tus necesidades.
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
mean_accuracy <- mean(accuracy_values)
std_accuracy <- sd(accuracy_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy))
}
result <- cross_validation(df, num_folds = 5)
# Imprimir la matriz de confusión, media y desviación estándar de la precisión
for (i in 1:length(result$EvaluationMetrics)) {
cat("Fold", i, ":\n")
print(result$EvaluationMetrics[[i]]$ConfusionMatrix)
cat("Accuracy:", result$EvaluationMetrics[[i]]$Accuracy, "\n\n")
}
result <- cross_validation(df, num_folds = 10)
# Imprimir la matriz de confusión, media y desviación estándar de la precisión
for (i in 1:length(result$EvaluationMetrics)) {
cat("Fold", i, ":\n")
print(result$EvaluationMetrics[[i]]$ConfusionMatrix)
cat("Accuracy:", result$EvaluationMetrics[[i]]$Accuracy, "\n\n")
}
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ altura + circ_tronco_cm + lat + long + seccion
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación (sensibilidad y especificidad)
true_positive <- confusion_matrix[2, 2]
false_negative <- confusion_matrix[2, 1]
true_negative <- confusion_matrix[1, 1]
false_positive <- confusion_matrix[1, 2]
sensitivity <- true_positive / (true_positive + false_negative)
specificity <- true_negative / (true_negative + false_positive)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Sensitivity = sensitivity,
Specificity = specificity
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
sensitivity_values <- sapply(evaluation_metrics, function(metric) metric$Sensitivity)
specificity_values <- sapply(evaluation_metrics, function(metric) metric$Specificity)
mean_sensitivity <- mean(sensitivity_values)
mean_specificity <- mean(specificity_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanSensitivity = mean_sensitivity,
MeanSpecificity = mean_specificity))
}
result <- cross_validation(df, num_folds = 10)
# Imprimir la matriz de confusión, media y desviación estándar de la precisión
for (i in 1:length(result$EvaluationMetrics)) {
cat("Fold", i, ":\n")
print(result$EvaluationMetrics[[i]]$ConfusionMatrix)
cat("Accuracy:", result$EvaluationMetrics[[i]]$Accuracy, "\n\n")
}
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ altura + circ_tronco_cm + lat + long + seccion
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación (por ejemplo, precisión, sensibilidad, especificidad, etc.)
# Puedes personalizar esto según tus necesidades.
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
mean_accuracy <- mean(accuracy_values)
std_accuracy <- sd(accuracy_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy))
}
result <- cross_validation(df, num_folds = 10)
# Imprimir la matriz de confusión, media y desviación estándar de la precisión
for (i in 1:length(result$EvaluationMetrics)) {
cat("Fold", i, ":\n")
print(result$EvaluationMetrics[[i]]$ConfusionMatrix)
cat("Accuracy:", result$EvaluationMetrics[[i]]$Accuracy, "\n\n")
}
result <- cross_validation(df, num_folds = 5)
# Imprimir la matriz de confusión, media y desviación estándar de la precisión
for (i in 1:length(result$EvaluationMetrics)) {
cat("Fold", i, ":\n")
print(result$EvaluationMetrics[[i]]$ConfusionMatrix)
cat("Accuracy:", result$EvaluationMetrics[[i]]$Accuracy, "\n\n")
}
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ especie + altura + circ_tronco_cm + lat + long + seccion
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación (por ejemplo, precisión, sensibilidad, especificidad, etc.)
# Puedes personalizar esto según tus necesidades.
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
mean_accuracy <- mean(accuracy_values)
std_accuracy <- sd(accuracy_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy))
}
result <- cross_validation(df, num_folds = 5)
cross_validation <- function(data, num_folds) {
# Asumiendo que 'data' es tu conjunto de datos completo
data$especie <- factor(data$especie, levels = unique(data$especie))
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ especie + altura + circ_tronco_cm + lat + long + seccion
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación (por ejemplo, precisión, sensibilidad, especificidad, etc.)
# Puedes personalizar esto según tus necesidades.
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
mean_accuracy <- mean(accuracy_values)
std_accuracy <- sd(accuracy_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy))
}
result <- cross_validation(df, num_folds = 5)
# Imprimir la matriz de confusión, media y desviación estándar de la precisión
for (i in 1:length(result$EvaluationMetrics)) {
cat("Fold", i, ":\n")
print(result$EvaluationMetrics[[i]]$ConfusionMatrix)
cat("Accuracy:", result$EvaluationMetrics[[i]]$Accuracy, "\n\n")
}
cat("Media de la precisión:", result$MeanAccuracy, "\n")
cat("Desviación estándar de la precisión:", result$StdAccuracy, "\n")
create_folds <- function(data, num_folds) {
# Obtener el número total de observaciones
total_observations <- nrow(data)
# Calcular el tamaño de cada fold
fold_size <- floor(total_observations / num_folds)
# Crear una lista para almacenar los índices de los folds
fold_list <- list()
# Dividir el dataframe en folds
for (i in 1:num_folds) {
start_index <- (i - 1) * fold_size + 1
end_index <- min(i * fold_size, total_observations)
# Obtener los índices de las observaciones para este fold
fold_indices <- start_index:end_index
# Agregar los índices a la lista
fold_list[[paste("Fold", i)]] <- fold_indices
}
return(fold_list)
}
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ especie + altura + circ_tronco_cm + lat + long + seccion
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación
TP <- confusion_matrix[2, 2] # Verdaderos positivos
TN <- confusion_matrix[1, 1] # Verdaderos negativos
FP <- confusion_matrix[1, 2] # Falsos positivos
FN <- confusion_matrix[2, 1] # Falsos negativos
accuracy <- (TP + TN) / sum(confusion_matrix)
precision <- TP / (TP + FP)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy,
Precision = precision,
Sensitivity = sensitivity,
Specificity = specificity
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
precision_values <- sapply(evaluation_metrics, function(metric) metric$Precision)
sensitivity_values <- sapply(evaluation_metrics, function(metric) metric$Sensitivity)
specificity_values <- sapply(evaluation_metrics, function(metric) metric$Specificity)
mean_accuracy <- mean(accuracy_values)
mean_precision <- mean(precision_values)
mean_sensitivity <- mean(sensitivity_values)
mean_specificity <- mean(specificity_values)
std_accuracy <- sd(accuracy_values)
std_precision <- sd(precision_values)
std_sensitivity <- sd(sensitivity_values)
std_specificity <- sd(specificity_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy,
MeanPrecision = mean_precision,
StdPrecision = std_precision,
MeanSensitivity = mean_sensitivity,
StdSensitivity = std_sensitivity,
MeanSpecificity = mean_specificity,
StdSpecificity = std_specificity))
}
result <- cross_validation(df, num_folds = 5)
cross_validation <- function(data, num_folds) {
# Crear una lista para almacenar las métricas de evaluación de cada fold
evaluation_metrics <- list()
# Crear los folds utilizando la función createFolds
set.seed(123) # Establecer una semilla para la reproducibilidad
folds <- createFolds(data$inclinacion_peligrosa, k = num_folds)
for (fold_index in 1:num_folds) {
train_indices <- unlist(folds[-fold_index])
test_indices <- folds[[fold_index]]
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
train_formula <- inclinacion_peligrosa ~ altura + circ_tronco_cm + lat + long + seccion
tree_model <- rpart(train_formula, data = train_data, method = "class")
p <- predict(tree_model, test_data, type = 'class')
# Calcular la matriz de confusión
confusion_matrix <- table(Real = test_data$inclinacion_peligrosa, Prediccion = p)
# Calcular las métricas de evaluación
TP <- confusion_matrix[2, 2] # Verdaderos positivos
TN <- confusion_matrix[1, 1] # Verdaderos negativos
FP <- confusion_matrix[1, 2] # Falsos positivos
FN <- confusion_matrix[2, 1] # Falsos negativos
accuracy <- (TP + TN) / sum(confusion_matrix)
precision <- TP / (TP + FP)
sensitivity <- TP / (TP + FN)
specificity <- TN / (TN + FP)
# Almacena las métricas en la lista
evaluation_metrics[[fold_index]] <- list(
ConfusionMatrix = confusion_matrix,
Accuracy = accuracy,
Precision = precision,
Sensitivity = sensitivity,
Specificity = specificity
)
}
# Calcular la media y desviación estándar de las métricas de evaluación
accuracy_values <- sapply(evaluation_metrics, function(metric) metric$Accuracy)
precision_values <- sapply(evaluation_metrics, function(metric) metric$Precision)
sensitivity_values <- sapply(evaluation_metrics, function(metric) metric$Sensitivity)
specificity_values <- sapply(evaluation_metrics, function(metric) metric$Specificity)
mean_accuracy <- mean(accuracy_values)
mean_precision <- mean(precision_values)
mean_sensitivity <- mean(sensitivity_values)
mean_specificity <- mean(specificity_values)
std_accuracy <- sd(accuracy_values)
std_precision <- sd(precision_values)
std_sensitivity <- sd(sensitivity_values)
std_specificity <- sd(specificity_values)
return(list(EvaluationMetrics = evaluation_metrics,
MeanAccuracy = mean_accuracy,
StdAccuracy = std_accuracy,
MeanPrecision = mean_precision,
StdPrecision = std_precision,
MeanSensitivity = mean_sensitivity,
StdSensitivity = std_sensitivity,
MeanSpecificity = mean_specificity,
StdSpecificity = std_specificity))
}
result <- cross_validation(df, num_folds = 5)
# Imprimir la matriz de confusión, media y desviación estándar de la precisión
for (i in 1:length(result$EvaluationMetrics)) {
cat("Fold", i, ":\n")
print(result$EvaluationMetrics[[i]]$ConfusionMatrix)
cat("Accuracy:", result$EvaluationMetrics[[i]]$Accuracy, "\n\n")
cat("Precision:", result$EvaluationMetrics[[i]]$Precision, "\n\n")
cat("Sensitivity:", result$EvaluationMetrics[[i]]$Sensitivity, "\n\n")
cat("Specificity:", result$EvaluationMetrics[[i]]$Specificity, "\n\n")
}
cat("Media de Accuracy:", result$MeanAccuracy, "\n")
cat("Desviación estándar de Accuracy:", result$StdAccuracy, "\n")
cat("Media de Precision:", result$MeanPrecision, "\n")
cat("Desviación estándar de Precision:", result$StdPrecision, "\n")
cat("Media de Sensitivity:", result$MeanSensitivity, "\n")
cat("Desviación estándar de Sensitivity:", result$StdSensitivity, "\n")
cat("Media de Specificity:", result$MeanSpecificity, "\n")
cat("Desviación estándar de Specificity:", result$StdSpecificity, "\n")
